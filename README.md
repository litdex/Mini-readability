Mini-readability
===================== 

##  Задача
Большинство веб-страниц сейчас перегружено всевозможной рекламой… Наша задача «вытащить» из веб-страницы только полезную информацию, отбросив весь «мусор» (навигацию, рекламу и тд).

## Решение
Так как практически у каждого сайта разная структура, то универсального способа для поиска контента нету. Так или иначе, мы должны будем прописывать свои настройки для каждого доменного имени. Легче всего это сделать в json
формате.

## Реализация программы
### Настройки

Для работы с настройками приложения мы создали абстрактный класс SettingsManager в модуле settings_manager.py, который автоматически выгружает настройки и проверяет их. Для его работы нужно указать в дочерних классах обязательные абстрактные переменные:

    self._filepath - содержит путь и название файла, куда стоит сохранить настройки
    self._DEFAULT_SETTINGS - содержит в себе шаблон настроек, который устанавливается по стандарт
    
А также обязательно следует переопределить абстрактные методы:

	_validate_settings(self, settings)-> bool: -> Проверяет настройки на соответствие стандартному шаблону
	set_settings() - для изменения настроек
	get_settings() - для получения настроек
  
На начальном этапе я создал 2 дочерних класса настроек:
1. Класс FormatSettingsManager, работающий с настройками формата возвращаемого результата
		Его настройки:  
    
        self._filepath = 'format_settings.json'

        self._DEFAULT_SETTINGS = {

        "str_len": 80,   -  Максимальная длина строки. Если ее превысить происходит перенос слова на следующую 
        "link_format": '{data} [{href}]', - Шаблон вывода ссылки {data} -> текст внутри ссылки, {href} -> сама ссылка

Предопределенные методы set_settings и get_settings позволяют установить и получить настройки по его ключу

2. Класс ParserSettingsManager, работающий с настройками парсера
		Его настройки:
    
			  self._filepath = 'parser_settings.json'

       			  self._DEFAULT_SETTINGS ={
            			'default': - Обычно тут будет название домена, но стандартные настройки для всех остальных сайтов будут называться        default
                			{
                    			"title_tags": ["h1"],   Список тагов для заголовка
                    			"title_atr": {},        Атрибуты для дополнительного описания тага (например title_tags = 'div', content_atr{'class':'title_class'})
                    			"content_tag": 'p',     Таг, предназначеный для отбора данных
                    			"content_atr": {},	Атрибуты для дополнительного описания тага (например content_tag = 'div', content_atr{'class':'paragraph'})
                			},

 Предопределенные методы set_settings и get_settings позволяют установить и получить настройки по доменному имени
 Дополнительный метод expand_settings(self, domain:str, kwargs) позволяет добавить новые настройки для нового доменного имени
 
### Загрузка документа

Загрузка HTML страницы реализованна в модуле html_downloader в классе HTMLDownloader
Для его реалзизации было использованно 2 библиотеки requests - для получения html по url, urllib -  используется для извлечения доменного имени
Он при вызове конструктора класса он автоматически получает доменное имя (для получения настроек) и html страницу

###  Получения полезной информации с документа

Для реализации основной функции программы был реализован класс HTMLParser, который извлекает данные из страницы html и преобразует их в текст.
Для его работы требуется:
1. библиотека BeautifulSoup, которая парсит страницу html и позволяет работать с ней
2. модуль queue, который содержит в себе структуру данных, необходимую для реализации алгоритма
3. модуль textwrap, который позволяет переносить текст по словам

В его конструкторе требуется вся html страница и словарь всех настроек. Он преобразует их в переменные класса.

#### Алгоритм его работы:

1. Он получает данные и сразу создает переменную self._soup = BeautifulSoup(html), с ней мы и будем проводить дальнейшие действия
2. Сразу во время вызова конструтора, мы удаляем все скрипты и стили из html
3. **В _parse_document, мы формируем нашу очередь со всем отформатированными по настройкам содержимым контента:**

        1. Находим все содержимое 1 указанных в параметрах тагах(включая их самих)
        2. Проходим по содержимому 1 
        3. Фильтруем таг контента по атрибутам
        4. Проходим по всему содержимому 2  в теге контента
        5. Если ссылка, то преобразуем ее по формату
        6. Добавляем параграф в очередь
        7. Фильтруем таг заголовка по атрибутам
        8. Проходим по всему содержимому 2  в теге заголовка
        9. Добавляем заголовок в очередь

4. Метод _parse_document вызывается в методе get_text().Метод get_text() преобразует очередь в текст, который переносится по словам с помощью метода textwrap.wrap() по настойкам.

### Сохранение статьи
Модуль content_manager содержит в себе стандартные методы для записи и получения информации из файла


### Консольное приложение
Сама консольная утилита реализованна в console_app.py, в который мы загружаем все реализованные ранее модули
Переменная content_place = 'content/{title}.txt' - место, куда будем сохранять файл и название, которое мы указываем в {title}
В ней есть метод create_filename, который из заголовка формирует название страницы (в дальнейшем это все можно вынести в отдельный модуль)

#### Алоритм:

1. Мы получаем настройки форматирования
2. Пользователь вводит url
3. Мы пробуем получить страницу html
4. Если получили, то  получаем настройки по домену
5. Создаем экземпляр класса HTMLParser, при вызове конструктора которого, указываем настройки, полученные на а) и г)
6. Получаем полезный текст и выводим его в консоль
7. Предлагаем пользователю сохранить его
8. Если пользователь соглашается, то получаем заголовок текста из метода create_filename() и сохраняем файл по заголовку

## Список сайтов, на которых была проведена проверка:

https://habr.com/ru/post/1000/ - струтура сайта очень своеобрана, поэтому пришлось добавлять настройки для habr.com,
 после находит всю информацию без проблем и сохраняет в файл, однако есть проблемы с оступами

https://lenta.ru/news/2019/07/18/soyuz/ - находит всю информацию без проблем и сохраняет в файл по стандартным настройкам

https://ria.ru/20190718/1556651419.html - находит всю  информацию без проблем и сохраняет текст после добавления данных в настройки 

https://kanobu.ru/news/tri-aktera-iz-igryi-prestolov-reshili-sami-sebya-vyidvinut-na-emmi-2019-415801/ - находит всю  информацию без проблем и сохраняет текст после добавления данных в настройки

## Дальнейшее улучшение:

Чтобы пользователь приложения мог сам изменять и расширять настройки приложения можно реализовать ему оконное приложения для работы, с помощью которого можно добавлять информацию для новых сайтов,
выбирать формат вывода и тд...

Для того, чтобы изменять и расширять настройками, пользователь так или иначе должен понимать структуру HTML страниц. Чтобы облегчить работу для пользователя, можно добавить фукнцию в менджере настроек,
чтобы он мог скопировать какой-либо полезный текст с сайта и программа бы автоматически могла узнать тег и его атрибуты, в которых этот текст содержится и таким образом позволяла бы пользователю заполнять 
настройки без особых знаний HTML.

Так же, для того, чтобы программа всегда работала корректно нужно ее качественно протестировать и исправить все проблемы, которые могут встретиться
